corrplot(newdatacor)
model <- lm(SalePrice ~ ., data=trainsub)
summary(model)
plot(model$residuals ~ model$fitted.values)
#extract variables that are significant and rerun model
sigvars <- data.frame(summary(model)$coef[summary(model)$coef[,4] <= .05, 4])
sigvars <- add_rownames(sigvars, "vars")
colist<-dplyr::pull(sigvars, vars)
idx <- match(colist, names(trainsub))
trainsub2 <- cbind(trainsub[,idx], trainsub['SalePrice'])
model2<-lm(SalePrice ~ ., data=trainsub2)
summary(model2)
setwd("~/Documents/GitHub/data621/Week2")
data = read.csv(file="data/classification-output-data.csv")
table(data)
table()
table(data)
summary(data)
table(data$pregnant)
data %>%
map( function(x) table(x) )
library(tidyverse)
data %>%
map( function(x) table(x) )
table(data$class)
table(data$scored.class)
table(data$class, data$scored.class)
CM[1,1]
CM = table(data$class, data$scored.class)
CM[1,1]
CM[1,2]
CM[1,2]
CM[2,1]
CM[1,2]
TP = CM[1,1]
TN = CM[2,2]
FP = CM[2,1]
FN = CM[1,2]
con.accurary <- function(x) {
CM = table(x$class, x$scored.class)
print(CM)
}
con.accuracy <- function(x) {
CM = table(x$class, x$scored.class)
print(CM)
}
con.accuracy(data)
CM
sum(CM)
con.accuracy <- function(x) {
CM = table(x$class, x$scored.class)
print(CM)
acc = (TP + TN)/sum(CM)
print(acc)
}
con.accuracy(data)
con.CER <- function(x) {
CM = table(x$class, x$scored.class)
print(CM)
CER = (FP + FN)/sum(CM)
print(CER)
}
con.CER(data)
con.CER(data) + con.accuracy(data) = 1
con.CER(data) + con.accuracy(data) == 1
con.prec <- function(x) {
CM = table(x$class, x$scored.class)
prec = TP / (TP + FP)
print(prec)
}
con.sens <- function(x) {
CM = table(x$class, x$scored.class)
sens = TP / (TP + FN)
print(sens)
}
con.accuracy <- function(x) {
CM = table(x$class, x$scored.class)
acc = (TP + TN)/sum(CM)
print(acc)
}
con.CER <- function(x) {
CM = table(x$class, x$scored.class)
CER = (FP + FN)/sum(CM)
print(CER)
}
con.accuracy(data) + con.CER(data) == 1
con.prec <- function(x) {
CM = table(x$class, x$scored.class)
prec = TP / (TP + FP)
print(prec)
}
con.sens <- function(x) {
CM = table(x$class, x$scored.class)
sens = TP / (TP + FN)
print(sens)
}
con.spec <- function(x) {
CM = table(x$class, x$scored.class)
spec = TP / (TP + FN)
print(spec)
}
con.F1 <- function(x) {
CM = table(x$class, x$scored.class)
F1 = (2*con.prec(x)*con.sens(x)) / (con.prec(x) + con.sens(x))
print(F1)
}
con.F1(data)
(2*0.1*0.1)/(0.1+0.1)
fastROC <- function(probs, class) {
class_sorted <- class[order(probs, decreasing=T)]
TPR <- cumsum(class_sorted) / sum(class)
FPR <- cumsum(class_sorted == 0) / sum(class == 0)
return(list(tpr=TPR, fpr=FPR))
}
# Helpful function adapted from: https://stat.ethz.ch/pipermail/r-help/2005-September/079872.html
fastAUC <- function(probs, class) {
x <- probs
y <- class
x1 = x[y==1]; n1 = length(x1);
x2 = x[y==0]; n2 = length(x2);
r = rank(c(x1,x2))
auc = (sum(r[1:n1]) - n1*(n1+1)/2) / n1 / n2
return(auc)
}
####################################################
### Some tests on random datasets
#small test dataset
probs <- runif(50000)
class <- sample(c(1,0), 50000, replace=T)
system.time(pROC_auc_results <- pROC::auc(pROC::roc(class, probs)))
#Elapsed: 17.906s
system.time(fast_auc_results <- fastAUC(probs, class))
#Elapsed: 0.022s
system.time(ROCR_auc_results <- ROCR::performance(ROCR::prediction(probs,class), measure="auc"))
#Elapsed: 0.208s
#Check that the results are the same -- True up to some float precision
abs(as.numeric(pROC_auc_results) - fast_auc_results) < 1e-12
abs(ROCR_auc_results@y.values[[1]] - fast_auc_results) < 1e-12
#Larger test dataset
probs <- runif(5e6)
class <- sample(c(1,0), 5e6, replace=T)
probs <- ifelse(class==1, probs+0.01, probs-0.01)
system.time(fast_auc_results <- fastAUC(probs, class))
#Elapsed: 3.687s
system.time(ROCR_auc_results <- ROCR::performance(ROCR::prediction(probs,class), measure="auc"))
#Elapsed: 16.006
#Check that the results are the same -- True up to some float precision
abs(ROCR_auc_results@y.values[[1]] - fast_auc_results) < 1e-12
set.seed(2529)
D.ex <- rbinom(200, size = 1, prob = .5)
M1 <- rnorm(200, mean = D.ex, sd = .65)
M2 <- rnorm(200, mean = D.ex, sd = 1.5)
test <- data.frame(D = D.ex, D.str = c("Healthy", "Ill")[D.ex + 1],
M1 = M1, M2 = M2, stringsAsFactors = FALSE)
test
ggplot(test, aes(d = D, m = M1)) + geom_roc()
d <- data.frame(id=1:100, ob=sample(c(1,0), 100, replace=T), m1=sample(seq(0,1,by=0.01), 100, replace=T))
d
View(d)
int <- 100
th <- seq(0,1, length=int)
roc.plot <- data.frame(sen=rep(NA,int), spe=rep(NA,int))
th
roc.plot
for (i in 1:int)
{
# get tn, tp, fn, fp
tn <- nrow(d[d[,3]th[i]&d[,2]==0,])
tp <- nrow(d[d[,3]>th[i]&d[,2]==1,])
# sensitivity, if sensitivty == 1, everything all positives are found
roc.plot[i,'sen'] <- tp/(tp+fn)
# specificity, if specificity == 1, all negatives are found
roc.plot[i,'spe'] <- tn/(tn+fp)
}
roc.plot <- data.frame(sen=rep(NA,int), spe=rep(NA,int))
View(roc.plot)
for (i in 1:int)
{
# get tn, tp, fn, fp
tn <- nrow(d[d[,3]th[i]&d[,2]==0,])
tp <- nrow(d[d[,3]>th[i]&d[,2]==1,])
# sensitivity, if sensitivty == 1, everything all positives are found
roc.plot[i,'sen'] <- tp/(tp+fn)
# specificity, if specificity == 1, all negatives are found
roc.plot[i,'spe'] <- tn/(tn+fp)
}
with(roc.plot, plot(1-spe, sen, type="l"))
## ROC - plot
d <- data.frame(id=1:100, ob=sample(c(1,0), 100, replace=T), m1=sample(seq(0,1,by=0.01), 100, replace=T))
# interval to calculate the threshold
int <- 100
th <- seq(0,1, length=int)
roc.plot <- data.frame(sen=rep(NA,int), spe=rep(NA,int))
for (i in 1:int)
{
# get tn, tp, fn, fp
tn <- nrow(d[d[,3]th[i]&d[,2]==0,])
tp <- nrow(d[d[,3]>th[i]&d[,2]==1,])
# sensitivity, if sensitivty == 1, everything all positives are found
roc.plot[i,'sen'] <- tp/(tp+fn)
# specificity, if specificity == 1, all negatives are found
roc.plot[i,'spe'] <- tn/(tn+fp)
}
with(roc.plot, plot(1-spe, sen, type="l"))
dim(data)
dim[1]
dim[[1]]
dim[1,
]
dim(1)
dim(2)
dim(data)
dim(data)[1]
View(roc.plot)
ifelse(data$scored.probability < 0.01, 0, 1)
ifelse(data$scored.probability < 0.02, 0, 1)
roc_curve <- function(ds, actual, pred.prob, threshold) {
tpr <- function(ds, actual, pred.prob, threshold) {
sum(select(class_data, get(pred.prob)) < threshold & select(class_data, get(actual)) == 0) / sum(select(class_data, get(actual)) == 0)
}
fpr <- function(ds, actual, pred.prob, threshold) {
sum(select(class_data, get(pred.prob)) < threshold & select(class_data, get(actual)) == 1) / sum(select(class_data, get(actual)) == 1)
}
roc <- data.frame(threshold = seq(0,1, by = 0.01), tpr=NA, fpr=NA)
roc$tpr <- sapply(roc$threshold, function(th) tpr(ds, actual, pred.prob, th))
roc$fpr <- sapply(roc$threshold, function(th) fpr(ds, actual, pred.prob, th))
roc$next_fpr <- lead(roc$fpr,1)
roc$area_curve <- (roc$next_fpr-roc$fpr) * roc$tpr
idx_threshold = which.min(abs(roc$threshold-0.5))
p_roc <- ggplot(roc, aes(fpr,tpr)) +
geom_line(color=rgb(0,0,1,alpha=0.3)) +
coord_fixed() +
geom_line(aes(threshold,threshold), color=rgb(0,0,1,alpha=0.5)) +
labs(title = sprintf("ROC")) + xlab("FPR") + ylab("TPR") +
geom_hline(yintercept=roc[idx_threshold,"tpr"], alpha=0.5, linetype="dashed") +
geom_vline(xintercept=roc[idx_threshold,"fpr"], alpha=0.5, linetype="dashed")
lst_roc_auc <- list(p_roc, sum(roc$area_curve, na.rm = TRUE))
return(lst_roc_auc)
}
ds<- data
actual <- "class"
pred.prob   <- "scored.probability"
threshold <- 0.5
par(mfrow=c(2,1))
roc_curve(ds, actual, pred.prob, 0.5)
class_data <- data
ds<- class_data
actual <- "class"
pred.prob   <- "scored.probability"
threshold <- 0.5
par(mfrow=c(2,1))
# output from our function
a<-roc_curve(ds, actual, pred.prob, 0.5)
head(data)
select(data, get("scored.probability"))
select(data, get("class"))
select(data, get(data$scored.probability))
select(data, get("scored.probability"))
sum(data$scored.probability) < 0.5
class_prob_df <- subset(data, select = c(scored.probability, class))
head(class_prob_df)
receiver_operating_characteristic <- function(df, intervals = 10000){
# Takes a 2-column df
# The  1st column is the predicted class probablity
# The 2nd column is the actual class (0 or 1)
# intervals creates the number of thresholds to use between 0 and 1
# Calcalutes the sensitivity & 1-specificity for all thresholds
# Prints the ROC curve plot & returns the AUC value
# AUC reference: https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve
thresholds <- seq(0, 1, by = 1/intervals)
sensitivity <- sort(sapply(thresholds, function(x) sensitivity_calc(df, threshold = x)))
one_minus_specificity <- sort(1 - sapply(thresholds, function(x) specificity_calc(df, threshold = x)))
#create plot
plot(sensitivity ~ one_minus_specificity, type = "s", xlim=c(0, 1), ylim=c(0, 1), main = "Custom Function")
abline(a = 0, b = 1)
AUC <- sum(diff(one_minus_specificity) * rollmean(sensitivity, 2))
AUC
}
receiver_operating_characteristic(class_prob_df)
receiver_operating_characteristic <- function(df, intervals = 10000){
# Takes a 2-column df
# The  1st column is the predicted class probablity
# The 2nd column is the actual class (0 or 1)
# intervals creates the number of thresholds to use between 0 and 1
# Calcalutes the sensitivity & 1-specificity for all thresholds
# Prints the ROC curve plot & returns the AUC value
# AUC reference: https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve
thresholds <- seq(0, 1, by = 1/intervals)
sensitivity <- sort(sapply(thresholds, function(x) con.sens(df, threshold = x)))
one_minus_specificity <- sort(1 - sapply(thresholds, function(x) con.spec(df, threshold = x)))
#create plot
plot(sensitivity ~ one_minus_specificity, type = "s", xlim=c(0, 1), ylim=c(0, 1), main = "Custom Function")
abline(a = 0, b = 1)
AUC <- sum(diff(one_minus_specificity) * rollmean(sensitivity, 2))
AUC
}
receiver_operating_characteristic(class_prob_df))
CM
table(x$class, x$scored.class)
table(data$class, data$scored.class)
con.sens <- function(x, threshold = 0.5) {
TP <- sum(df[, 1] > threshold & df[, 2] == 1)
FN <- sum(df[, 1] <= threshold & df[, 2] == 1)
TP/(TP + FN)
}
con.spec <- function(x, threshold = 0.5) {
TN <- sum(df[, 1] <= threshold & df[, 2] == 0)
FP <- sum(df[, 1] > threshold & df[, 2] == 0)
TN/(TN + FP)
}
con.sens(data)
data[,:-2]
data[,-2:]
dim(data)
data[,9:10]
data = read.csv(file="data/classification-output-data.csv")
data = subset(data, select = c(scored.class, class))
con.sens(data)
con.sens <- function(x, threshold = 0.5) {
TP <- sum(df[, 1] > threshold & df[, 2] == 1)
FN <- sum(df[, 1] <= threshold & df[, 2] == 1)
TP/(TP + FN)
}
con.spec <- function(x, threshold = 0.5) {
TN <- sum(df[, 1] <= threshold & df[, 2] == 0)
FP <- sum(df[, 1] > threshold & df[, 2] == 0)
TN/(TN + FP)
}
con.sens <- function(x, threshold = 0.5) {
TP <- sum(x[, 1] > threshold & x[, 2] == 1)
FN <- sum(x[, 1] <= threshold & x[, 2] == 1)
TP/(TP + FN)
}
con.spec <- function(x, threshold = 0.5) {
TN <- sum(x[, 1] <= threshold & x[, 2] == 0)
FP <- sum(x[, 1] > threshold & x[, 2] == 0)
TN/(TN + FP)
}
con.sens(data)
head(data)
View(data)
class_prob_df <- subset(data, select = c(scored.probability, class))
origdata = read.csv(file="data/classification-output-data.csv")
data = subset(origdata, select = c(scored.class, class))
class_prob_df <- subset(origdata, select = c(scored.probability, class))
receiver_operating_characteristic <- function(df, intervals = 0.01){
# Takes a 2-column df
# The  1st column is the predicted class probablity
# The 2nd column is the actual class (0 or 1)
# intervals creates the number of thresholds to use between 0 and 1
# Calcalutes the sensitivity & 1-specificity for all thresholds
# Prints the ROC curve plot & returns the AUC value
# AUC reference: https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve
thresholds <- seq(0, 1, by = intervals)
sensitivity <- sort(sapply(thresholds, function(x) sensitivity_calc(df, threshold = x)))
one_minus_specificity <- sort(1 - sapply(thresholds, function(x) specificity_calc(df, threshold = x)))
#create plot
plot(sensitivity ~ one_minus_specificity, type = "s", xlim=c(0, 1), ylim=c(0, 1), main = "Custom Function")
abline(a = 0, b = 1)
AUC <- sum(diff(one_minus_specificity) * rollmean(sensitivity, 2))
AUC
}
receiver_operating_characteristic(class_prod_df)
myROC <- function(df, intervals = 0.01){
# Takes a 2-column df
# The  1st column is the predicted class probablity
# The 2nd column is the actual class (0 or 1)
# intervals creates the number of thresholds to use between 0 and 1
# Calcalutes the sensitivity & 1-specificity for all thresholds
# Prints the ROC curve plot & returns the AUC value
# AUC reference: https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve
thresholds <- seq(0, 1, by = intervals)
sensitivity <- sort(sapply(thresholds, function(x) con.sens(df, threshold = x)))
one_minus_specificity <- sort(1 - sapply(thresholds, function(x) con.spec(df, threshold = x)))
#create plot
plot(sensitivity ~ one_minus_specificity, type = "s", xlim=c(0, 1), ylim=c(0, 1), main = "Custom Function")
abline(a = 0, b = 1)
AUC <- sum(diff(one_minus_specificity) * rollmean(sensitivity, 2))
AUC
}
myROC(class_prob_df)
install.packages("zoo")
library(zoo)
myROC <- function(df, intervals = 0.01){
# Takes a 2-column df
# The  1st column is the predicted class probablity
# The 2nd column is the actual class (0 or 1)
# intervals creates the number of thresholds to use between 0 and 1
# Calcalutes the sensitivity & 1-specificity for all thresholds
# Prints the ROC curve plot & returns the AUC value
# AUC reference: https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve
thresholds <- seq(0, 1, by = intervals)
sensitivity <- sort(sapply(thresholds, function(x) con.sens(df, threshold = x)))
one_minus_specificity <- sort(1 - sapply(thresholds, function(x) con.spec(df, threshold = x)))
#create plot
plot(sensitivity ~ one_minus_specificity, type = "s", xlim=c(0, 1), ylim=c(0, 1), main = "Custom Function")
abline(a = 0, b = 1)
AUC <- sum(diff(one_minus_specificity) * rollmean(sensitivity, 2))
AUC
}
myROC(class_prob_df)
myROC(data)
data_SPC <- subset(origdata, select = c(scored.probability, class))
myROC(data_SPC)
con.F1 <- function(x, threshold = 0.5) {
CM = table(x$class, x$scored.class)
F1 = (2*con.prec(x)*con.sens(x)) / (con.prec(x) + con.sens(x))
print(F1)
}
con.F1(data)
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(dplyr)
library(purrr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(FactoMineR)
library(VIF)
library(knitr)
library(kableExtra)
library(Hmisc)
library(zoo)
# read data
origdata = read.csv(file="data/classification-output-data.csv")
data = subset(origdata, select = c(scored.class, class))
CM = table(data$scored.class, data$class)
TP = CM[1,1]
TN = CM[2,2]
FP = CM[2,1]
FN = CM[1,2]
con.accuracy <- function(x) {
CM = table(x$scored.class, x$class)
acc = (TP + TN)/sum(CM)
}
con.CER <- function(x) {
CM = table(x$class, x$scored.class)
CER = (FP + FN)/sum(CM)
}
con.accuracy(data) + con.CER(data) == 1
con.prec <- function(x) {
CM = table(x$class, x$scored.class)
prec = TP / (TP + FP)
}
con.sens <- function(x, threshold = 0.5) {
TP <- sum(x[, 1] > threshold & x[, 2] == 1)
FN <- sum(x[, 1] <= threshold & x[, 2] == 1)
TP/(TP + FN)
}
con.spec <- function(x, threshold = 0.5) {
TN <- sum(x[, 1] <= threshold & x[, 2] == 0)
FP <- sum(x[, 1] > threshold & x[, 2] == 0)
TN/(TN + FP)
}
con.F1 <- function(x, threshold = 0.5) {
CM = table(x$class, x$scored.class)
F1 = (2*con.prec(x)*con.sens(x)) / (con.prec(x) + con.sens(x))
print(F1)
}
#bounds if the class and scored class are perfect equal to each other then FP and FN would be 0 and Prec and Sens each would be 1
#the max would be 2*1*1 / 2 or 1
#if the model doesn't predict anything
# then closer to 0
data_SPC <- subset(origdata, select = c(scored.probability, class))
myROC <- function(df, intervals = 0.01){
# AUC reference: https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve
thresholds <- seq(0, 1, by = intervals)
sensitivity <- sort(sapply(thresholds, function(x) con.sens(df, threshold = x)))
one_minus_specificity <- sort(1 - sapply(thresholds, function(x) con.spec(df, threshold = x)))
#create plot
plot(sensitivity ~ one_minus_specificity, type = "s", xlim=c(0, 1), ylim=c(0, 1), main = "Custom Function")
abline(a = 0, b = 1)
AUC <- sum(diff(one_minus_specificity) * rollmean(sensitivity, 2))
AUC
}
con.F1(data)
myROC(data_SPC)
install.packages("caret")
install.packages("pROC")
library(caret)
install.packages("recipes")
library(caret)
library(pROC)
library(caret)
install.packages("recipes")
library(caret)
install.packages("caret",
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
library(caret)
confusionMatrix(data = data$scored.class, reference = data$class)
confusionMatrix(data$scored.class, data$class)
head(data)
confusionMatrix(origdata$scored.class, origdata$class)
str(origdata)
summary(origdata)
confusionMartrix(CM)
confusionMatrix(CM)
roc(origdata$scored.class, origdata$class)
roc(scored.class ~ class, origdata)
plot.roc(origdata$scored.class, origdata$class)
data_SPC <- subset(origdata, select = c(scored.probability, class))
myROC <- function(df, intervals = 0.01){
thresholds <- seq(0, 1, by = intervals)
sensitivity <- sort(sapply(thresholds, function(x) con.sens(df, threshold = x)))
one_minus_specificity <- sort(1 - sapply(thresholds, function(x) con.spec(df, threshold = x)))
#create plot
plot(sensitivity ~ one_minus_specificity, type = "s", xlim=c(0, 1), ylim=c(0, 1), main = "Custom Function")
abline(a = 0, b = 1)
sum(diff(one_minus_specificity) * rollmean(sensitivity, 2))
}
myROC(data_SPC)
